{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "7dd476b71131c51c0392ebcd09843bf3c4de4313",
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from multiprocessing.dummy import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.externals import joblib\n",
    "\n",
    "from skimage.morphology import binary_opening, disk, label\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "13428284fe657373fa036d52f6e9e78aba3d141a",
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "5144bc9049d81ecfc54d7dc9540391080f069a38",
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_dpath = 'C:/Users/DELL/Desktop/Airbus Data/train_v2'\n",
    "test_dpath = 'C:/Users/DELL/Desktop/Airbus Data/test_v2'\n",
    "\n",
    "anno_fpath = 'C:/Users/DELL/Desktop/Airbus Data/train_ship_segmentations_v2.csv'\n",
    "bst_model_fpath = 'model/bst_unet.model'\n",
    "\n",
    "sample_submission_fpath = '.model/bst_unet.model'\n",
    "submission_fpath = 'sample_submission_v2.csv'\n",
    "\n",
    "original_img_size = (768, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "bffb8dfe8377ddbcb12d7bed539c63f024c9e3bc",
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00003e153.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001124c7.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000155de5.jpg</td>\n",
       "      <td>264661 17 265429 33 266197 33 266965 33 267733...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000194a2d.jpg</td>\n",
       "      <td>360486 1 361252 4 362019 5 362785 8 363552 10 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000194a2d.jpg</td>\n",
       "      <td>51834 9 52602 9 53370 9 54138 9 54906 9 55674 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageId                                      EncodedPixels\n",
       "0  00003e153.jpg                                                NaN\n",
       "1  0001124c7.jpg                                                NaN\n",
       "2  000155de5.jpg  264661 17 265429 33 266197 33 266965 33 267733...\n",
       "3  000194a2d.jpg  360486 1 361252 4 362019 5 362785 8 363552 10 ...\n",
       "4  000194a2d.jpg  51834 9 52602 9 53370 9 54138 9 54906 9 55674 ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annos = pd.read_csv(anno_fpath)\n",
    "annos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4190d22a2fd7fd7583eaaf39ef7ccb0dd13e16f5"
   },
   "source": [
    "# Data smapling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "35b3c7aa2ddd5babf832bdb7b9337f5a54e136cb"
   },
   "outputs": [],
   "source": [
    "annos['EncodedPixels_flag'] = annos['EncodedPixels'].map(lambda v: 1 if isinstance(v, str) else 0)\n",
    "imgs = annos.groupby('ImageId').agg({'EncodedPixels_flag': 'sum'}).reset_index().rename(columns={'EncodedPixels_flag': 'ships'})\n",
    "\n",
    "imgs_w_ships = imgs[imgs['ships'] > 0]\n",
    "imgs_wo_ships = imgs[imgs['ships'] == 0].sample(20000, random_state=69278)\n",
    "\n",
    "selected_imgs = pd.concat((imgs_w_ships, imgs_wo_ships))\n",
    "selected_imgs['has_ship'] = selected_imgs['ships'] > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "92ab6e7ebc3c900aed9565031c5deee35fd313a0"
   },
   "source": [
    "### Train set and validation set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "e430864a5ed8eb4b515ab106187e3cccb229d1fb"
   },
   "outputs": [],
   "source": [
    "train_imgs, val_imgs = train_test_split(selected_imgs, test_size=0.15, stratify=selected_imgs['has_ship'], random_state=69278)\n",
    "\n",
    "train_fnames = train_imgs['ImageId'].values\n",
    "val_fnames = val_imgs['ImageId'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a710c9f79a8e84d458fb86a75ddce5cf607035b7"
   },
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "47d0ffbdf5ffcc44f43786faa7c88c980df8094b"
   },
   "outputs": [],
   "source": [
    "_, train_fnames = train_test_split(train_fnames, test_size=0.1, random_state=69278)\n",
    "_, val_fnames = train_test_split(val_fnames, test_size=0.1, random_state=69278)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1af6e488a6a1e4e1d6e80f583b3e5f66de27989",
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Data Transformation for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "5abf9ec299e16419f7f5a4c7fc78ba0a226422ae",
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 img_dpath,\n",
    "                 img_fnames,\n",
    "                 img_transform,\n",
    "                 mask_encodings=None,\n",
    "                 mask_size=None,\n",
    "                 mask_transform=None):\n",
    "        self.img_dpath = img_dpath\n",
    "        self.img_fnames = img_fnames\n",
    "        self.img_transform = img_transform\n",
    "\n",
    "        self.mask_encodings = mask_encodings\n",
    "        self.mask_size = mask_size\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # https://github.com/pytorch/vision/issues/9#issuecomment-304224800\n",
    "        seed = np.random.randint(2147483647)\n",
    "\n",
    "        fname = self.img_fnames[i]\n",
    "        fpath = os.path.join(self.img_dpath, fname)\n",
    "        img = Image.open(fpath)\n",
    "        if self.img_transform is not None:\n",
    "            random.seed(seed)\n",
    "            img = self.img_transform(img)\n",
    "\n",
    "        if self.mask_encodings is None:\n",
    "            return img, fname\n",
    "\n",
    "        if self.mask_size is None or self.mask_transform is None:\n",
    "            raise ValueError('If mask_dpath is not None, mask_size and mask_transform must not be None.')\n",
    "\n",
    "        mask = np.zeros(self.mask_size, dtype=np.uint8)\n",
    "        if self.mask_encodings[fname][0] == self.mask_encodings[fname][0]: # NaN doesn't equal to itself\n",
    "            for encoding in self.mask_encodings[fname]:\n",
    "                mask += rle_decode(encoding, self.mask_size)\n",
    "        mask = np.clip(mask, 0, 1)\n",
    "\n",
    "        mask = Image.fromarray(mask)\n",
    "\n",
    "        random.seed(seed)\n",
    "        mask = self.mask_transform(mask)\n",
    "\n",
    "        return img, torch.from_numpy(np.array(mask, dtype=np.int64))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d2d9402003303b9705cdba3c1a1c6bceede40f8",
    "ein.tags": "worksheet-0",
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## RLE  Methods for image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "3a40035b7712eda0d67011a2101d951df6cd08ba",
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "hidden": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    ends = starts + lengths\n",
    "    im = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        im[lo:hi] = 1\n",
    "    return im.reshape(shape).T\n",
    "\n",
    "def rle_encode(im):\n",
    "    pixels = im.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    runs[::2] -= 1\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "ae090b2d6ac782fe715ff74578b81ac4eed9081c"
   },
   "outputs": [],
   "source": [
    "def get_mask_encodings(annos, fnames):\n",
    "    a = annos[annos['ImageId'].isin(fnames)]\n",
    "    return a.groupby('ImageId')['EncodedPixels'].apply(lambda x: x.tolist()).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f8ec0abd2e349bd52688490db3c65009faf6af3",
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "2af67d8b888cd922d1520f17471f5c09b276bfef",
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def conv1x1(in_channels, out_channels, groups=1):\n",
    "    return nn.Conv2d(in_channels,\n",
    "                     out_channels,\n",
    "                     kernel_size=1,\n",
    "                     groups=groups,\n",
    "                     stride=1)\n",
    "\n",
    "def conv3x3(in_channels, out_channels, stride=1, padding=1, bias=True, groups=1):\n",
    "    return nn.Conv2d(in_channels,\n",
    "                     out_channels,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=padding,\n",
    "                     bias=bias,\n",
    "                     groups=groups)\n",
    "\n",
    "def upconv2x2(in_channels, out_channels, mode='transpose'):\n",
    "    if mode == 'transpose':\n",
    "        return nn.ConvTranspose2d(in_channels,\n",
    "                                  out_channels,\n",
    "                                  kernel_size=2,\n",
    "                                  stride=2)\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "            conv1x1(in_channels, out_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "11151befdcb4aae62792a55a22a30cfae9541b33",
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class DownConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, pooling=True):\n",
    "        super(DownConv, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.pooling = pooling\n",
    "\n",
    "        self.conv1 = conv3x3(self.in_channels, self.out_channels)\n",
    "        self.conv2 = conv3x3(self.out_channels, self.out_channels)\n",
    "\n",
    "        if self.pooling:\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        before_pool = x\n",
    "        if self.pooling:\n",
    "            x = self.pool(x)\n",
    "        return x, before_pool\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,merge_mode='concat',up_mode='transpose'):\n",
    "        super(UpConv, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.merge_mode = merge_mode\n",
    "        self.up_mode = up_mode\n",
    "\n",
    "        self.upconv = upconv2x2(self.in_channels,self.out_channels,mode=self.up_mode)\n",
    "\n",
    "        if self.merge_mode == 'concat':\n",
    "            self.conv1 = conv3x3(2*self.out_channels,self.out_channels)\n",
    "        else:\n",
    "            # num of input channels to conv2 is same\n",
    "            self.conv1 = conv3x3(self.out_channels, self.out_channels)\n",
    "\n",
    "        self.conv2 = conv3x3(self.out_channels, self.out_channels)\n",
    "\n",
    "    def forward(self, from_down, from_up):\n",
    "        from_up = self.upconv(from_up)\n",
    "        if self.merge_mode == 'concat':\n",
    "            x = torch.cat((from_up, from_down), 1)\n",
    "        else:\n",
    "            x = from_up + from_down\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "703a1a903a9a97f1e55e201e0e5e899a4989631b",
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3, depth=5,\n",
    "                 start_filts=64, up_mode='transpose',\n",
    "                 merge_mode='concat'):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        if up_mode in ('transpose', 'upsample'):\n",
    "            self.up_mode = up_mode\n",
    "        else:\n",
    "            raise ValueError(\"\\\"{}\\\" is not a valid mode for upsampling. Only \\\"transpose\\\" and \\\"upsample\\\" are allowed.\".format(up_mode))\n",
    "        if merge_mode in ('concat', 'add'):\n",
    "            self.merge_mode = merge_mode\n",
    "        else:\n",
    "            raise ValueError(\"\\\"{}\\\" is not a valid mode for merging up and down paths. Only \\\"concat\\\" and \"\"\\\"add\\\" are allowed.\".format(up_mode))\n",
    "\n",
    "        # NOTE: up_mode 'upsample' is incompatible with merge_mode 'add'\n",
    "        if self.up_mode == 'upsample' and self.merge_mode == 'add':\n",
    "            raise ValueError(\"up_mode \\\"upsample\\\" is incompatible with merge_mode \\\"add\\\" at the moment because it doesn't make sense to use \"\n",
    "                             \"nearest neighbour to reduce depth channels (by half).\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.start_filts = start_filts\n",
    "        self.depth = depth\n",
    "\n",
    "        self.down_convs = []\n",
    "        self.up_convs = []\n",
    "\n",
    "        # create the encoder pathway and add to a list\n",
    "        for i in range(depth):\n",
    "            ins = self.in_channels if i == 0 else outs\n",
    "            outs = self.start_filts*(2**i)\n",
    "            pooling = True if i < depth-1 else False\n",
    "\n",
    "            down_conv = DownConv(ins, outs, pooling=pooling)\n",
    "            self.down_convs.append(down_conv)\n",
    "\n",
    "        # create the decoder pathway and add to a list\n",
    "        # - careful! decoding only requires depth-1 blocks\n",
    "        for i in range(depth-1):\n",
    "            ins = outs\n",
    "            outs = ins // 2\n",
    "            up_conv = UpConv(ins, outs, up_mode=up_mode,merge_mode=merge_mode)\n",
    "            self.up_convs.append(up_conv)\n",
    "\n",
    "        self.conv_final = conv1x1(outs, self.num_classes)\n",
    "\n",
    "        # add the list of modules to current module\n",
    "        self.down_convs = nn.ModuleList(self.down_convs)\n",
    "        self.up_convs = nn.ModuleList(self.up_convs)\n",
    "\n",
    "        self.reset_params()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_normal(m.weight)\n",
    "            nn.init.constant(m.bias, 0)\n",
    "\n",
    "\n",
    "    def reset_params(self):\n",
    "        for i, m in enumerate(self.modules()):\n",
    "            self.weight_init(m)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_outs = []\n",
    "\n",
    "        # encoder pathway, save outputs for merging\n",
    "        for i, module in enumerate(self.down_convs):\n",
    "            x, before_pool = module(x)\n",
    "            encoder_outs.append(before_pool)\n",
    "\n",
    "        for i, module in enumerate(self.up_convs):\n",
    "            before_pool = encoder_outs[-(i+2)]\n",
    "            x = module(before_pool, x)\n",
    "\n",
    "        # No softmax is used. This means you need to use\n",
    "        # nn.CrossEntropyLoss is your training script,\n",
    "        # as this module includes a softmax already.\n",
    "        x = self.conv_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f1cec8c14a2806eeb37cd6ed6f28b1710f2aee3d",
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "c53cf6b6f46ec6a53b505d5ca6c7ea81359f6922",
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class param:\n",
    "    img_size = (80, 80)\n",
    "    bs = 8\n",
    "    num_workers = 4\n",
    "    lr = 0.0001\n",
    "    epochs = 30\n",
    "    unet_depth = 5\n",
    "    unet_start_filters = 8\n",
    "    log_interval = 70 # less then len(train_dl)\n",
    "\n",
    "channel_means = (0.20166926, 0.28220195, 0.31729624)\n",
    "channel_stds = (0.20769505, 0.18813899, 0.16692209)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "8d602c2fdcfe2ce53207ca0a32ac5a4e0a15df77",
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_1112\\3513193218.py:98: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_1112\\3513193218.py:99: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(m.bias, 0)\n"
     ]
    }
   ],
   "source": [
    "train_tfms = transforms.Compose([transforms.Resize(param.img_size),\n",
    "                                 transforms.RandomRotation(360),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(channel_means, channel_stds)])\n",
    "val_tfms = transforms.Compose([transforms.Resize(param.img_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(channel_means, channel_stds)])\n",
    "mask_tfms = transforms.Compose([transforms.Resize(param.img_size),\n",
    "                                transforms.RandomRotation(360)])\n",
    "\n",
    "train_dl = DataLoader(ImgDataset(train_dpath,\n",
    "                                 train_fnames,\n",
    "                                 train_tfms,\n",
    "                                 get_mask_encodings(annos, train_fnames),\n",
    "                                 original_img_size,\n",
    "                                 mask_tfms),\n",
    "                      batch_size=param.bs,\n",
    "                      shuffle=True,\n",
    "                      pin_memory=False,\n",
    "                      num_workers=param.num_workers)\n",
    "val_dl = DataLoader(ImgDataset(train_dpath,\n",
    "                               val_fnames,\n",
    "                               val_tfms,\n",
    "                               get_mask_encodings(annos, val_fnames),\n",
    "                               original_img_size,\n",
    "                               mask_tfms),\n",
    "                    batch_size=param.bs,\n",
    "                    shuffle=False,\n",
    "                    pin_memory=False,\n",
    "                    num_workers=param.num_workers)\n",
    "\n",
    "model = UNet(2,\n",
    "             depth=param.unet_depth,\n",
    "             start_filts=param.unet_start_filters,\n",
    "             merge_mode='concat').cpu()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=param.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "ae5cddfee556e3f213074f5903d2256b225647c7",
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_loss(dl, model):\n",
    "    loss = 0\n",
    "    for X, y in dl:\n",
    "        X, y = Variable(X).cuda(), Variable(y).cpu()\n",
    "        output = model(X)\n",
    "        loss += F.cross_entropy(output, y).data[0]\n",
    "    loss = loss / len(dl)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f789fd785a3d01245125677bfb64fac695c5c0b2",
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "iters = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "it = 0\n",
    "min_loss = np.inf\n",
    "\n",
    "os.makedirs(os.path.dirname(bst_model_fpath), exist_ok=True)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(param.epochs):\n",
    "    for i, (X, y) in enumerate(train_dl):\n",
    "        X = Variable(X).cpu()  # [N, 1, H, W]\n",
    "        y = Variable(y).cpu()  # [N, H, W] with class indices (0, 1)\n",
    "        output = model(X)  # [N, 2, H, W]\n",
    "        loss = F.cross_entropy(output, y)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if (i + 1) % param.log_interval == 0:\n",
    "            it += param.log_interval * param.bs\n",
    "            iters.append(it)\n",
    "            train_losses.append(loss.data[0])\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = get_loss(val_dl, model)\n",
    "            model.train()\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            if val_loss < min_loss:\n",
    "                torch.save(model.state_dict(), bst_model_fpath)\n",
    "\n",
    "model.eval()\n",
    "val_loss = get_loss(val_dl, model)\n",
    "if val_loss < min_loss:\n",
    "    torch.save(model.state_dict(), bst_model_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b4e8822d838abf84a753525dfccb9e1d3c380539",
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(iters, train_losses)\n",
    "plt.plot(iters, val_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8a07f32e0356017e52c2227482f8f124899e7151",
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce93ea3eec9d75875e180b828422c7a007d8ec32",
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(sample_submission_fpath)\n",
    "test_fnames = sample_submission['ImageId'].values\n",
    "\n",
    "test_dl = DataLoader(ImgDataset(test_dpath,\n",
    "                                test_fnames,\n",
    "                                val_tfms),\n",
    "                     batch_size=param.bs,\n",
    "                     shuffle=False,\n",
    "                     pin_memory=torch.cuda.is_available(),\n",
    "                     num_workers=param.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e62a1e186340b66d8087d1710be4b7aad68bc57"
   },
   "outputs": [],
   "source": [
    "submission = {'ImageId': [], 'EncodedPixels': []}\n",
    "\n",
    "model.eval()\n",
    "for X, fnames in test_dl:\n",
    "    X = Variable(X).cpu()\n",
    "    output = model(X)\n",
    "    for i, fname in enumerate(fnames):\n",
    "        mask = F.sigmoid(output[i, 0]).data.cpu().numpy()\n",
    "        mask = binary_opening(mask > 0.5, disk(2))\n",
    "        mask = Image.fromarray(mask.astype(np.uint8)).resize(original_img_size)\n",
    "        mask = np.array(mask).astype(np.bool)\n",
    "\n",
    "        labels = label(mask)\n",
    "        encodings = [rle_encode(labels == k) for k in np.unique(labels[labels > 0])]\n",
    "        if len(encodings) > 0:\n",
    "            for encoding in encodings:\n",
    "                submission['ImageId'].append(fname)\n",
    "                submission['EncodedPixels'].append(encoding)\n",
    "        else:\n",
    "            submission['ImageId'].append(fname)\n",
    "            submission['EncodedPixels'].append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f1b43aa9fe4d7f7f18af2de0c949b91a9f32aec",
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d6966a092134651d58a1507c33a66cdeb28879ed",
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(submission, columns=['ImageId', 'EncodedPixels'])\n",
    "submission_df.to_csv('submission_1.csv', index=False)\n",
    "submission_df.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "name": "unet_pytorch.ipynb",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
